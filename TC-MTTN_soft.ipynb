{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5b8ba-ca76-4170-9dfd-c1a352d2b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.nn.functional import normalize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fc07a94-13ca-4945-934b-dbe92fcd5175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f2790-c256-4cb0-b4d7-b88504d0d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 400\n",
    "data = pd.read_csv(\"Two_Train_15s_25_50.csv\")\n",
    "temp_1_tr = np.array(data[\"Temp_1_Train\"])/100\n",
    "heat_1_tr = np.array(data[\"Heat_1_Train\"])/100\n",
    "\n",
    "\n",
    "temp_2_tr = np.array(data[\"Temp_2_Train\"])/100\n",
    "heat_2_tr = np.array(data[\"Heat_2_Train\"])/100\n",
    "\n",
    "out_1_tr = temp_1_tr[start+2:-1].reshape(-1,1)\n",
    "temp_1_tr_pr = temp_1_tr[start:-3].reshape(-1,1)\n",
    "heat_1_tr_pr = heat_1_tr[start+1:-2].reshape(-1,1)\n",
    "heat_1_tr = heat_1_tr[start+2:-1].reshape(-1,1)\n",
    "temp_1_tr = temp_1_tr[start+1:-2].reshape(-1,1)\n",
    "\n",
    "out_2_tr = temp_2_tr[start+2:-1].reshape(-1,1)\n",
    "temp_2_tr_pr = temp_2_tr[start:-3].reshape(-1,1)\n",
    "heat_2_tr_pr = heat_2_tr[start+1:-2].reshape(-1,1)\n",
    "heat_2_tr = heat_2_tr[start+2:-1].reshape(-1,1)\n",
    "temp_2_tr = temp_2_tr[start+1:-2].reshape(-1,1)\n",
    "\n",
    "delta_1_tr = np.concatenate(((temp_1_tr - temp_1_tr_pr).reshape(-1,1), (heat_1_tr - heat_1_tr_pr).reshape(-1,1)), axis=1)\n",
    "delta_2_tr = np.concatenate(((temp_2_tr - temp_2_tr_pr).reshape(-1,1), (heat_2_tr - heat_2_tr_pr).reshape(-1,1)), axis=1)\n",
    "\n",
    "delta_tr = np.concatenate((delta_1_tr, delta_2_tr), axis=1)\n",
    "output_tr = np.concatenate((out_1_tr, out_2_tr), axis=1)\n",
    "input_tr = np.concatenate((temp_1_tr_pr, heat_1_tr_pr, temp_2_tr_pr, heat_2_tr_pr), axis=1)\n",
    "curr_tr = np.concatenate((temp_1_tr, temp_2_tr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "522b8624-b29a-4c14-a58e-34d6e18f8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Tensor(curr_tr, input_tr, delta_tr, output_tr):\n",
    "    curr_tr = torch.tensor(curr_tr).float().to(device).reshape(-1,2).requires_grad_(True)\n",
    "    input_tr = torch.tensor(input_tr).float().to(device).reshape(-1,4).requires_grad_(True)\n",
    "    delta_tr = torch.tensor(delta_tr).float().to(device).reshape(-1,4).requires_grad_(True)\n",
    "    output_tr = torch.tensor(output_tr).float().to(device).reshape(-1,2)\n",
    "    return curr_tr, input_tr, delta_tr, output_tr\n",
    "curr_tr, input_tr, delta_tr, output_tr = convert_Tensor(curr_tr, input_tr, delta_tr, output_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdd385-814a-4ced-b988-04b39929a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taylor_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Taylor_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 4)\n",
    "        self.fc2_1 = nn.Linear(1, 1)\n",
    "        self.fc2_2 = nn.Linear(1, 1)\n",
    "        self.fc2_3 = nn.Linear(1, 1)\n",
    "        self.fc2_4 = nn.Linear(1, 1)\n",
    "\n",
    "        self.fc3 = nn.Linear(4, 4)\n",
    "        self.fc3_1 = nn.Linear(1, 1)\n",
    "        self.fc3_2 = nn.Linear(1, 1)\n",
    "        self.fc3_3 = nn.Linear(1, 1)\n",
    "        self.fc3_4 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward_1(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x_1 = self.fc2_1(x[:, 0].reshape(-1, 1))\n",
    "        y_1 = self.fc2_2(x[:, 1].reshape(-1, 1))\n",
    "        z_1 = self.fc2_3(x[:, 2].reshape(-1, 1))\n",
    "        t_1 = torch.zeros_like(x[:, 3].reshape(-1, 1))\n",
    "\n",
    "        return torch.cat((x_1, y_1, z_1, t_1), dim=1), y_1, z_1\n",
    "\n",
    "    def forward_2(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x_1 = self.fc3_1(x[:, 0].reshape(-1, 1))\n",
    "        y_1 = torch.zeros_like(x[:, 1].reshape(-1, 1))\n",
    "        z_1 = self.fc3_3(x[:, 2].reshape(-1, 1))\n",
    "        t_1 = self.fc3_4(x[:, 3].reshape(-1, 1))\n",
    "\n",
    "        return torch.cat((x_1, y_1, z_1, t_1), dim=1), y_1, z_1\n",
    "\n",
    "    def calculate(self, curr_tr, input_tr, delta_tr):\n",
    "        x1, y_1, z_1 = self.forward_1(input_tr)\n",
    "        x1 = torch.mul(x1, delta_tr)\n",
    "        x1 = torch.sum(x1, dim=1, keepdim=True)\n",
    "\n",
    "        x2, y_2, z_2 = self.forward_2(input_tr)\n",
    "        x2 = torch.mul(x2, delta_tr)\n",
    "        x2 = torch.sum(x2, dim=1, keepdim=True)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)  \n",
    "        x = curr_tr + x\n",
    "\n",
    "        return x, y_1, z_1, y_2, z_2\n",
    "    \n",
    "    def forward(self, curr_tr, input_tr, delta_tr):\n",
    "        return self.calculate(curr_tr, input_tr, delta_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2e597-7a60-42da-83f8-52c9055ba232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_steps(model, curr_tr, input_tr, delta_tr, output_tr, num_predict=4):\n",
    "    r2_loop = []\n",
    "\n",
    "    t_next = model(curr_tr[0:-num_predict, :], input_tr[0:-num_predict, :], delta_tr[0:-num_predict, :])\n",
    "    r2 = r2_score(output_tr[:-num_predict, :].to(\"cpu\").detach().numpy(), t_next.to(\"cpu\").detach().numpy())\n",
    "    r2_loop.append(r2)\n",
    "    \n",
    "    delta = t_next - curr_tr[0:-num_predict, :]\n",
    "    delta_copy = torch.empty_like(delta_tr[0:-num_predict, :])\n",
    "    delta_copy[:, 0] = delta[:, 0]\n",
    "    delta_copy[:, 1] = delta_tr[1:1-num_predict, 1]\n",
    "    delta_copy[:, 2] = delta[:, 1]\n",
    "    delta_copy[:, 3] = delta_tr[1:1-num_predict, 3]\n",
    "    \n",
    "    t_next_2 = model(t_next, input_tr[1:1-num_predict, :], delta_copy)\n",
    "    r2 = r2_score(output_tr[1:1-num_predict].to(\"cpu\").detach().numpy(), t_next_2.to(\"cpu\").detach().numpy())\n",
    "    r2_loop.append(r2)\n",
    "\n",
    "    for i in range(2, num_predict):\n",
    "        delta = t_next_2 - t_next\n",
    "        delta_copy = torch.empty_like(delta_tr[i:i-num_predict, :]) \n",
    "        delta_copy[:, 0] = delta[:, 0]\n",
    "        delta_copy[:, 1] = delta_tr[i:i-num_predict, 1]\n",
    "        delta_copy[:, 2] = delta[:, 1]\n",
    "        delta_copy[:, 3] = delta_tr[i:i-num_predict, 3]\n",
    "        \n",
    "        t_next = t_next_2\n",
    "        t_next_2 = model(t_next_2, input_tr[i:i-num_predict, :], delta_copy)\n",
    "        r2 = r2_score(output_tr[i:i-num_predict].to(\"cpu\").detach().numpy(), t_next_2.to(\"cpu\").detach().numpy())\n",
    "        r2_loop.append(r2)\n",
    "    \n",
    "    return r2_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92895c-66a9-43b0-ba62-d9c07d60da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, curr_tr, input_tr, delta_tr, output_tr, scheduler=None, epochs=20000, patience=1000, threshold=1e-10, print_interval=5000):\n",
    "    prev_loss = float('inf')\n",
    "    no_improvement_counter = 0\n",
    "    loss_array = []\n",
    "    last_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, y_1, z_1, y_2, z_2 = model(curr_tr, input_tr, delta_tr)\n",
    "        \n",
    "        term_1 = F.relu(-y_1)\n",
    "        term_2 = F.relu(-y_2)\n",
    "        term_3 = F.relu(-z_1)\n",
    "        term_4 = F.relu(-z_2)\n",
    "        \n",
    "        alpha = 0.05  \n",
    "        regularized_term = alpha * (term_1.mean() + term_2.mean() + term_3.mean() + term_4.mean())\n",
    "        \n",
    "        loss = criterion(out, output_tr) + regularized_term\n",
    "        loss_array.append(loss.item())\n",
    "  \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        last_loss = loss\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        if epoch % print_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.9f}')\n",
    "        if abs(prev_loss - loss.item()) < threshold:\n",
    "            no_improvement_counter += 1\n",
    "        else:\n",
    "            no_improvement_counter = 0 \n",
    "        prev_loss = loss.item()\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} due to no significant improvement.')\n",
    "            break\n",
    "        if loss.item() < 0.000000000308385432:\n",
    "            print(f'Early stopping at epoch {epoch+1} due to very low loss.')\n",
    "            print(\"min of loss function is\", min(loss_array))\n",
    "            break\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3283d5-598e-4ab8-89b5-f438df358f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12000\n",
    "patience = 2000 \n",
    "lr = 0.001\n",
    "num_test = 1\n",
    "threshold = 1e-15\n",
    "decay_factor = 1. \n",
    "decay_step_size = 1000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbd0fb-56b3-45dc-b5c9-6dec2a0d2f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12000], Loss: 0.106505089\n",
      "Epoch [5001/12000], Loss: 0.000011538\n",
      "Epoch [10001/12000], Loss: 0.000009538\n",
      "9.562442e-06\n",
      "Epoch [1/12000], Loss: 0.011210811\n",
      "Epoch [5001/12000], Loss: 0.000008146\n",
      "Epoch [10001/12000], Loss: 0.000008238\n",
      "8.162831e-06\n",
      "Epoch [1/12000], Loss: 0.040222429\n",
      "Epoch [5001/12000], Loss: 0.000009656\n",
      "Epoch [10001/12000], Loss: 0.000008227\n",
      "Epoch [1/12000], Loss: 0.000437165\n",
      "Epoch [5001/12000], Loss: 0.000008622\n",
      "Epoch [10001/12000], Loss: 0.000008628\n",
      "Epoch [1/12000], Loss: 0.025787162\n",
      "Epoch [5001/12000], Loss: 0.000008059\n",
      "Epoch [10001/12000], Loss: 0.000008227\n",
      "Epoch [1/12000], Loss: 0.000161716\n",
      "Epoch [5001/12000], Loss: 0.000008240\n",
      "Epoch [10001/12000], Loss: 0.000008136\n",
      "Epoch [1/12000], Loss: 0.001450838\n",
      "Epoch [5001/12000], Loss: 0.000009250\n",
      "Epoch [10001/12000], Loss: 0.000009065\n",
      "Epoch [1/12000], Loss: 0.033112850\n",
      "Epoch [5001/12000], Loss: 0.000008335\n",
      "Epoch [10001/12000], Loss: 0.000009785\n",
      "Epoch [1/12000], Loss: 0.015919808\n",
      "Epoch [5001/12000], Loss: 0.000008294\n",
      "Epoch [10001/12000], Loss: 0.000008381\n",
      "Epoch [1/12000], Loss: 0.014673824\n",
      "Epoch [5001/12000], Loss: 0.000008780\n",
      "Epoch [10001/12000], Loss: 0.000009233\n",
      "Epoch [1/12000], Loss: 0.061226938\n",
      "Epoch [5001/12000], Loss: 0.000008465\n",
      "Epoch [10001/12000], Loss: 0.000010835\n",
      "Epoch [1/12000], Loss: 0.040501002\n",
      "Epoch [5001/12000], Loss: 0.000008205\n",
      "Epoch [10001/12000], Loss: 0.000008035\n",
      "Epoch [1/12000], Loss: 0.040010415\n",
      "Epoch [5001/12000], Loss: 0.000008969\n",
      "Epoch [10001/12000], Loss: 0.000008292\n",
      "Epoch [1/12000], Loss: 0.031056693\n",
      "Epoch [5001/12000], Loss: 0.000008346\n",
      "Epoch [10001/12000], Loss: 0.000008313\n",
      "Epoch [1/12000], Loss: 0.070472173\n",
      "Epoch [5001/12000], Loss: 0.000009424\n",
      "Epoch [10001/12000], Loss: 0.000008257\n",
      "Epoch [1/12000], Loss: 0.084648490\n",
      "Epoch [5001/12000], Loss: 0.000012942\n",
      "Epoch [10001/12000], Loss: 0.000011419\n",
      "Epoch [1/12000], Loss: 0.049034044\n",
      "Epoch [5001/12000], Loss: 0.000008963\n",
      "Epoch [10001/12000], Loss: 0.000014506\n",
      "Epoch [1/12000], Loss: 0.051864866\n",
      "Epoch [5001/12000], Loss: 0.000010938\n",
      "Epoch [10001/12000], Loss: 0.000008468\n",
      "Epoch [1/12000], Loss: 0.068568490\n",
      "Epoch [5001/12000], Loss: 0.000008551\n",
      "Epoch [10001/12000], Loss: 0.000011903\n",
      "Epoch [1/12000], Loss: 0.040167239\n",
      "Epoch [5001/12000], Loss: 0.000010743\n",
      "Epoch [10001/12000], Loss: 0.000008216\n",
      "Epoch [1/12000], Loss: 0.028833173\n",
      "Epoch [5001/12000], Loss: 0.000009929\n",
      "Epoch [10001/12000], Loss: 0.000008283\n",
      "Epoch [1/12000], Loss: 0.039583903\n",
      "Epoch [5001/12000], Loss: 0.000014736\n",
      "Epoch [10001/12000], Loss: 0.000008748\n",
      "Epoch [1/12000], Loss: 0.042915799\n",
      "Epoch [5001/12000], Loss: 0.000008499\n",
      "Epoch [10001/12000], Loss: 0.000013191\n",
      "Epoch [1/12000], Loss: 0.014712008\n",
      "Epoch [5001/12000], Loss: 0.000009392\n",
      "Epoch [10001/12000], Loss: 0.000016830\n",
      "Epoch [1/12000], Loss: 0.117877036\n",
      "Epoch [5001/12000], Loss: 0.000008084\n",
      "Epoch [10001/12000], Loss: 0.000010743\n",
      "Epoch [1/12000], Loss: 0.022620279\n",
      "Epoch [5001/12000], Loss: 0.000009408\n",
      "Epoch [10001/12000], Loss: 0.000016592\n",
      "Epoch [1/12000], Loss: 0.099385269\n",
      "Epoch [5001/12000], Loss: 0.000008427\n",
      "Epoch [10001/12000], Loss: 0.000011676\n",
      "Epoch [1/12000], Loss: 0.067326799\n",
      "Epoch [5001/12000], Loss: 0.000023486\n",
      "Epoch [10001/12000], Loss: 0.000009480\n",
      "Epoch [1/12000], Loss: 0.028661473\n",
      "Epoch [5001/12000], Loss: 0.000009081\n",
      "Epoch [10001/12000], Loss: 0.000014582\n",
      "Epoch [1/12000], Loss: 0.014056668\n",
      "Epoch [5001/12000], Loss: 0.000012277\n",
      "Epoch [10001/12000], Loss: 0.000023597\n",
      "Epoch [1/12000], Loss: 0.033253357\n",
      "Epoch [5001/12000], Loss: 0.000018284\n",
      "Epoch [10001/12000], Loss: 0.000008461\n",
      "Epoch [1/12000], Loss: 0.047525935\n",
      "Epoch [5001/12000], Loss: 0.000014332\n",
      "Epoch [10001/12000], Loss: 0.000027462\n",
      "Epoch [1/12000], Loss: 0.028086504\n",
      "Epoch [5001/12000], Loss: 0.000008808\n",
      "Epoch [10001/12000], Loss: 0.000010074\n",
      "Epoch [1/12000], Loss: 0.010794849\n",
      "Epoch [5001/12000], Loss: 0.000010718\n",
      "Epoch [10001/12000], Loss: 0.000017922\n",
      "Epoch [1/12000], Loss: 0.047069862\n",
      "Epoch [5001/12000], Loss: 0.000008628\n",
      "Epoch [10001/12000], Loss: 0.000011917\n",
      "Epoch [1/12000], Loss: 0.093377843\n",
      "Epoch [5001/12000], Loss: 0.000011473\n",
      "Epoch [10001/12000], Loss: 0.000018801\n",
      "Epoch [1/12000], Loss: 0.056244340\n",
      "Epoch [5001/12000], Loss: 0.000013942\n",
      "Epoch [10001/12000], Loss: 0.000023573\n",
      "Epoch [1/12000], Loss: 0.033962321\n",
      "Epoch [5001/12000], Loss: 0.000036738\n",
      "Epoch [10001/12000], Loss: 0.000008408\n",
      "Epoch [1/12000], Loss: 0.033006348\n",
      "Epoch [5001/12000], Loss: 0.000013194\n",
      "Epoch [10001/12000], Loss: 0.000024594\n",
      "Epoch [1/12000], Loss: 0.028279722\n",
      "Epoch [5001/12000], Loss: 0.000020019\n",
      "Epoch [10001/12000], Loss: 0.000008187\n",
      "Epoch [1/12000], Loss: 0.053837337\n",
      "Epoch [5001/12000], Loss: 0.000008663\n",
      "Epoch [10001/12000], Loss: 0.000009678\n",
      "Epoch [1/12000], Loss: 0.043786280\n",
      "Epoch [5001/12000], Loss: 0.000008507\n",
      "Epoch [10001/12000], Loss: 0.000017026\n",
      "7.9472975e-06\n",
      "Epoch [1/12000], Loss: 0.038761310\n",
      "Epoch [5001/12000], Loss: 0.000012600\n",
      "Epoch [10001/12000], Loss: 0.000016726\n",
      "Epoch [1/12000], Loss: 0.001058673\n",
      "Epoch [5001/12000], Loss: 0.000033809\n",
      "Epoch [10001/12000], Loss: 0.000042985\n",
      "Epoch [1/12000], Loss: 0.085659198\n",
      "Epoch [5001/12000], Loss: 0.000014578\n",
      "Epoch [10001/12000], Loss: 0.000029289\n",
      "Epoch [1/12000], Loss: 0.040018246\n",
      "Epoch [5001/12000], Loss: 0.000008280\n",
      "Epoch [10001/12000], Loss: 0.000008366\n",
      "Epoch [1/12000], Loss: 0.005812292\n",
      "Epoch [5001/12000], Loss: 0.000008272\n",
      "Epoch [10001/12000], Loss: 0.000008325\n",
      "Epoch [1/12000], Loss: 0.023701908\n",
      "Epoch [5001/12000], Loss: 0.000011328\n",
      "Epoch [10001/12000], Loss: 0.000011569\n",
      "Epoch [1/12000], Loss: 0.000646325\n",
      "Epoch [5001/12000], Loss: 0.000033554\n",
      "Epoch [10001/12000], Loss: 0.000034148\n",
      "Epoch [1/12000], Loss: 0.076224416\n",
      "Epoch [5001/12000], Loss: 0.000029814\n",
      "Epoch [10001/12000], Loss: 0.000028664\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "mse.append(1.2362453e-02)\n",
    "learning_rates = np.arange(0.001, 0.1, 0.002)\n",
    "for lr in learning_rates:\n",
    "    model_Taylor = Taylor_Net().to(device)\n",
    "    optimizer = optim.Adam(model_Taylor.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=decay_step_size, gamma=decay_factor)\n",
    "    \n",
    "    train_model(model=model_Taylor, \n",
    "                optimizer=optimizer, \n",
    "                criterion=nn.MSELoss(), \n",
    "                curr_tr=curr_tr, \n",
    "                input_tr=input_tr, \n",
    "                delta_tr=delta_tr, \n",
    "                output_tr=output_tr, \n",
    "                scheduler=scheduler, \n",
    "                epochs=epochs, \n",
    "                patience=patience, \n",
    "                threshold=threshold, \n",
    "                print_interval=5000)\n",
    "    \n",
    "    t_next_test,_,_,_,_ = model_Taylor(curr_tr, input_tr, delta_tr)\n",
    "    current_mse = mean_squared_error(output_tr.to(\"cpu\").detach().numpy(), t_next_test.to(\"cpu\").detach().numpy())\n",
    "   \n",
    "    if current_mse > -10:\n",
    "        if current_mse < min(mse):\n",
    "            mse.append(current_mse)\n",
    "            torch.save(model_Taylor.state_dict(), f'Two_Taylor_1st_loss_TC_test_15s_non_25_50_400.pth')\n",
    "            print(current_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1589f0-1c33-4cdb-a3b3-5ad8a1ea7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Two_Taylor_1st_loss_TC_test_15s_non_25_50_400.pth'\n",
    "model_Taylor = Taylor_Net().to(device)\n",
    "model_Taylor.load_state_dict(torch.load(model_path))\n",
    "\n",
    "r2_results = predict_steps(model_Taylor, curr_tr, input_tr, delta_tr, output_tr, num_predict=4)\n",
    "print(r2_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
